{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c320907a-4bd9-40b6-b42f-74cef7691351",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db20f-d245-4e87-bad3-6fa0bfbedc21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afaca70-74f2-4fcf-9a83-c01cfa46ad33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from ipylogging import DisplayHandler, HTMLFormatter\n",
    "\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.addHandler(handler)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade3e0-8048-4e7f-a0dd-dc651fa4a12e",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d761f18c-b3b8-4dab-84f7-9f9b56e8431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('../data')\n",
    "cache_dir_github = data_dir.joinpath('github')\n",
    "bots_dataset_path = data_dir.joinpath('bots-dataset.csv')\n",
    "bots_issues_dir = data_dir.joinpath('bots-issues')\n",
    "\n",
    "#github_token = open('../gh-token.txt','r').readlines()[0].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5713359-ff37-41f7-bb5b-061ce9f30868",
   "metadata": {},
   "source": [
    "## Step 1 - Collect bots from [Golzadeh et al.](https://zenodo.org/record/4000388)'s dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35605f-4609-44a4-a509-324ae27d6e32",
   "metadata": {},
   "source": [
    "Download dataset from [Golzadeh et al.](https://zenodo.org/record/4000388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17e3ed-7f81-49e1-b7fa-6760e93d4ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "url_bot_dataset = \"https://zenodo.org/record/4000388/files/groundtruthbots.csv.gz\"\n",
    "path_bot_dataset = data_dir.joinpath('groundtruthbots.csv')\n",
    "\n",
    "gz_path, _ = urllib.request.urlretrieve(url_bot_dataset)\n",
    "with gzip.open(gz_path, \"rb\") as f_in, open(path_bot_dataset, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2369-880b-4120-8d2f-36ac77dab317",
   "metadata": {},
   "source": [
    "Extract bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd30b-d15c-4ad0-82e6-04965add1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "\n",
    "bot_dataset = load_csv_dataset(path_bot_dataset)\n",
    "bot_dataset = [{'account': row['account']} for row in bot_dataset if row['type'] == 'Bot']\n",
    "save_csv_dataset(bots_dataset_path, data=bot_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0c8d7-1d45-424f-a368-11f9cca98495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Collect the number of issues per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3475c3-72d8-4fad-bd90-6aec7b409b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "from minirig import GHRequests\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    try:\n",
    "        bot['issue_count'] = gh_api.get_number_issues_involving_user(bot['account'], force=True)\n",
    "    except:\n",
    "        bot['issue_count'] = 'na'\n",
    "\n",
    "bot_dataset.sort(reverse=True, key=lambda x: -1 if x['issue_count'] == 'na' else x['issue_count'])\n",
    "save_csv_dataset('../data/new-bot-dataset.csv', data=bot_dataset, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21537d-2805-4dca-881b-319aef79fa96",
   "metadata": {},
   "source": [
    "## Step 3 - Download issues for each bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8834e8-c314-416a-8157-da09afcaaa75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        \n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        \n",
    "        if not bot_issue_path.joinpath('json').exists():    \n",
    "            full_issue = gh_api.get_issue_info(issue['number'], owner, project)  \n",
    "            bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "vaadin_bot = [x for x in bot_dataset if x['account'] == 'vaadin-bot']\n",
    "\n",
    "for bot in vaadin_bot:\n",
    "    \n",
    "    for issue in gh_api.get_issues_involving_user(bot['account'], force= True):\n",
    "        \n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        \n",
    "        if not bot_issue_path.joinpath('json').exists():    \n",
    "            full_issue = gh_api.get_issue_info(issue['number'], owner, project)  \n",
    "            bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99eec13",
   "metadata": {},
   "source": [
    "## Step 4 - Download comments for each issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/','')).joinpath('comments')\n",
    "        bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        comments = gh_api.get_comments_per_issue(issue['number'], owner, project, force = True)           \n",
    "        with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "            json.dump(comments, f)\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        print(bot_issue_path)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc35d9",
   "metadata": {},
   "source": [
    "## Step 6 - Labeling sections with [Li et al.](http://doi.org/10.1007/s10664-022-10128-3)'s Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f73c907-9685-461d-bf74-db8883835506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 19:27:42.477399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/mambauser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from model_li2022_emse import *\n",
    "nltk.download('punkt')\n",
    "model1_li2022_emse = Model1_IssueTracker_Li2022_ESEM('../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5', \n",
    "                 '../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-word_embedding_file.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c15f61-2b02-4727-8099-ee59a07a2dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-05-10 19:32:49,628</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] k8s-ci-robot: 306/52160"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "bots_dataset = pd.read_csv('../data/bots-dataset-labeling-management.csv',delimiter=';')\n",
    "for i in bots_dataset.index:\n",
    "    cnt = 1\n",
    "    if bots_dataset['label_finished'][i] == 'yes':\n",
    "        continue\n",
    "    bot = bots_dataset['account'][i]\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                issue_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue)\n",
    "                \n",
    "                with open(issue_path.joinpath('json')) as f:\n",
    "                    try:\n",
    "                        issue_file = json.load(f)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                if 'td-label-li2022-emse' not in issue_file.keys():\n",
    "                \n",
    "                    issue_file['td-label-li2022-emse'] = model1_li2022_emse.label(issue_file['body'])\n",
    "\n",
    "                    with open(issue_path.joinpath('json'), 'w') as f:\n",
    "                        json.dump(issue_file, f)\n",
    "                \n",
    "                with open(issue_path.joinpath('comments').joinpath('json')) as f:\n",
    "                    comments = json.load(f)\n",
    "                    \n",
    "                for c in comments:\n",
    "                    if 'td-label-li2022-emse' not in c.keys():\n",
    "                        c['td-label-li2022-emse'] = model1_li2022_emse.label(c['body'])\n",
    "                \n",
    "                with open(issue_path.joinpath('comments').joinpath('json'),'w') as f:\n",
    "                    json.dump(comments, f)\n",
    "                    \n",
    "                clear_output()\n",
    "                logging.info(f'{bot}: {cnt}/{bots_dataset[\"issue_count\"][i]}')\n",
    "                cnt += 1\n",
    "    bots_dataset['label_finished'][i] = 'yes'\n",
    "    bots_dataset.to_csv('../data/bots-dataset-labeling-management.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd2e6b",
   "metadata": {},
   "source": [
    "## Step 7 - Labeling sections with [Li et al.](http://doi.org/10.1109/TSE.2022.3224378)'s Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from model_li2022_tse import *\n",
    "\n",
    "v = Model1_IssueTracker_Li2022_TSE('../model/satd-issue_mul.hdf5', '../model/fasttext_issue_300.bin')\n",
    "\n",
    "issues_dataset = pd.read_csv('../data/dataset-backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda391",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset['td-label-li-tse'] = '-'\n",
    "len_dataset = len(issues_dataset)\n",
    "for j, i in enumerate(issues_dataset.index):\n",
    "    issues_dataset['td-label'][i] = v.classify_prob_comment(issues_dataset['text'][i])\n",
    "    clear_output()\n",
    "    log.info(f'Handling {i+1} out {len_dataset} lines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
