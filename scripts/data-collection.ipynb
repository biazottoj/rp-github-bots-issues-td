{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c320907a-4bd9-40b6-b42f-74cef7691351",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db20f-d245-4e87-bad3-6fa0bfbedc21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afaca70-74f2-4fcf-9a83-c01cfa46ad33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from ipylogging import DisplayHandler, HTMLFormatter\n",
    "\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.addHandler(handler)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade3e0-8048-4e7f-a0dd-dc651fa4a12e",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d761f18c-b3b8-4dab-84f7-9f9b56e8431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('../data')\n",
    "cache_dir_github = data_dir.joinpath('github')\n",
    "bots_dataset_path = data_dir.joinpath('bots-dataset.csv')\n",
    "bots_issues_dir = data_dir.joinpath('bots-issues')\n",
    "\n",
    "#github_token = open('../gh-token.txt','r').readlines()[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5713359-ff37-41f7-bb5b-061ce9f30868",
   "metadata": {},
   "source": [
    "## Step 1 - Collect bots from [Golzadeh et al.](https://zenodo.org/record/4000388)'s dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35605f-4609-44a4-a509-324ae27d6e32",
   "metadata": {},
   "source": [
    "Download dataset from [Golzadeh et al.](https://zenodo.org/record/4000388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17e3ed-7f81-49e1-b7fa-6760e93d4ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "url_bot_dataset = \"https://zenodo.org/record/4000388/files/groundtruthbots.csv.gz\"\n",
    "path_bot_dataset = data_dir.joinpath('groundtruthbots.csv')\n",
    "\n",
    "gz_path, _ = urllib.request.urlretrieve(url_bot_dataset)\n",
    "with gzip.open(gz_path, \"rb\") as f_in, open(path_bot_dataset, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2369-880b-4120-8d2f-36ac77dab317",
   "metadata": {},
   "source": [
    "Extract bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd30b-d15c-4ad0-82e6-04965add1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "\n",
    "bot_dataset = load_csv_dataset(path_bot_dataset)\n",
    "bot_dataset = [{'account': row['account']} for row in bot_dataset if row['type'] == 'Bot']\n",
    "save_csv_dataset(bots_dataset_path, data=bot_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0c8d7-1d45-424f-a368-11f9cca98495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Collect the number of issues per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3475c3-72d8-4fad-bd90-6aec7b409b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "from minirig import GHRequests\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    try:\n",
    "        bot['issue_count'] = gh_api.get_number_issues_involving_user(bot['account'])\n",
    "    except:\n",
    "        bot['issue_count'] = 'na'\n",
    "\n",
    "bot_dataset.sort(reverse=True, key=lambda x: -1 if x['issue_count'] == 'na' else x['issue_count'])\n",
    "save_csv_dataset(bots_dataset_path, data=bot_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21537d-2805-4dca-881b-319aef79fa96",
   "metadata": {},
   "source": [
    "## Step 3 - Download issues for each bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8834e8-c314-416a-8157-da09afcaaa75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        bot_issue_path = bot_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        full_issue = gh_api.get_issue_info(issue['number'], owner, project)       \n",
    "        if not bot_issue_path.joinpath('json').exists():\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99eec13",
   "metadata": {},
   "source": [
    "## Step 4 - Download comments for each issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/','')).joinpath('comments')\n",
    "        bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        comments = gh_api.get_comments_per_issue(issue['number'], owner, project, force = True)           \n",
    "        with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "            json.dump(comments, f)\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        print(bot_issue_path)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98aa33",
   "metadata": {},
   "source": [
    "## Step 5 - Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6f2d30c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-04-20 20:09:04,146</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] renovate-bot: Handling 43 out of 755"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'bot_issues_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11548\\1194065726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                         \u001b[0mbot_issue_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbot_issues_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'account'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'html_url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://github.com/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'comments'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                         \u001b[0mcomments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgh_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_comments_per_issue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot_issue_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bot_issues_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset, save_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from helpers import *\n",
    "    \n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "dataset_name = 'dataset-backup.csv'\n",
    "dataset = []  \n",
    "\n",
    "headers =['bot',\n",
    "            'owner',\n",
    "            'project',\n",
    "            'issue',\n",
    "            'text',\n",
    "            'type',\n",
    "            'author-login',\n",
    "            'open-date',\n",
    "            'state',\n",
    "            'close-date',\n",
    "            'closed-by',\n",
    "            'n-comments',\n",
    "            'td-label']\n",
    "\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        rows = []\n",
    "        owner, project = (issue['repository_url'].replace('https://api.github.com/repos/', '').split('/'))\n",
    "        issue_path = bots_issues_dir.joinpath(f\"{bot['account']}/{owner}/{project}/issues/{issue['number']}\")\n",
    "        with open(issue_path.joinpath('json'), 'r') as f:\n",
    "            issue_json = json.load(f)\n",
    "        rows.append(create_dataset_row(bot['account'], issue_json, text_section='body', is_comment=False, comment_number=None,owner=owner, project=project))\n",
    "        if issue['comments'] > 0:\n",
    "            with open(issue_path.joinpath('comments').joinpath('json'), 'r') as f:\n",
    "                comments = json.load(f)\n",
    "            for i,comment in enumerate(comments):\n",
    "                rows.append(create_dataset_row(bot['account'], comment, is_comment=True, comment_number=i, owner=owner, project=project))\n",
    "        dataset.extend(rows)\n",
    "        cnt += 1\n",
    "    save_csv_dataset(data=dataset,header=headers, filename = data_dir.joinpath(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc35d9",
   "metadata": {},
   "source": [
    "## Step 6 - Labeling sections with [Li et al.](http://doi.org/10.1007/s10664-022-10128-3)'s Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "327b85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from model_li2022_emse import *\n",
    "\n",
    "model1_li2022_emse = Model1_IssueTracker_Li2022_ESEM('../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5', \n",
    "                 '../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-word_embedding_file.bin')\n",
    "\n",
    "issues_dataset = pd.read_csv('../data/dataset-backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6af58b7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-04-21 07:19:49,627</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] Handling 50157 out 333850 lines"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-04-21 07:19:53,873</span> [<span style=\"font-weight: bold; color: crimson\">ERROR</span>] Internal Python error in the inspect module.\n",
       "Below is the traceback from this internal error.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\joao\\AppData\\Local\\Temp\\ipykernel_11548\\2361851197.py\", line 4, in <module>\n",
      "    issues_dataset['td-label'][i] = model1_li2022_emse.predict(str(issues_dataset['text'][i]))\n",
      "  File \"C:\\Users\\joao\\Documents\\bots-td-daniel-version\\scripts\\model_li2022_emse.py\", line 84, in predict\n",
      "    y_pred = self._model.predict(input_x)\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 2317, in predict\n",
      "    data_handler = data_adapter.get_data_handler(\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1579, in get_data_handler\n",
      "    return DataHandler(*args, **kwargs)\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 1259, in __init__\n",
      "    self._adapter = adapter_cls(\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 347, in __init__\n",
      "    dataset = self.slice_inputs(indices_dataset, inputs)\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\", line 388, in slice_inputs\n",
      "    dataset = dataset.map(grab_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2296, in map\n",
      "    return ParallelMapDataset(\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 5555, in __init__\n",
      "    variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n",
      "  File \"C:\\Users\\joao\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 6184, in parallel_map_dataset_v2\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\inspect.py\", line 1505, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 182, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\linecache.py\", line 46, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"C:\\tools\\Anaconda3\\lib\\tokenize.py\", line 392, in open\n",
      "    buffer = _builtin_open(filename, 'rb')\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-04-21 07:19:54,123</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] \n",
       "Unfortunately, your original traceback can not be constructed.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11548\\2361851197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missues_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0missues_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'td-label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1_li2022_emse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0missues_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\bots-td-daniel-version\\scripts\\model_li2022_emse.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, comment)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# Make predictions using the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0my_pred_bool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   2318\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1578\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m         self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1260\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrab_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2296\u001b[1;33m       return ParallelMapDataset(\n\u001b[0m\u001b[0;32m   2297\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5554\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5555\u001b[1;33m     variant_tensor = gen_dataset_ops.parallel_map_dataset_v2(\n\u001b[0m\u001b[0;32m   5556\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   6183\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6184\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6185\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ParallelMapDatasetV2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2076\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "issues_dataset['td-label-li-emse'] = '-'\n",
    "len_dataset = len(issues_dataset)\n",
    "for j, i in enumerate(issues_dataset.index):\n",
    "    issues_dataset['td-label'][i] = model1_li2022_emse.predict(str(issues_dataset['text'][i]))\n",
    "    clear_output()\n",
    "    log.info(f'Handling {i+1} out {len_dataset} lines')\n",
    "issues_dataset.to_csv('../data/dataset-labeled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd2e6b",
   "metadata": {},
   "source": [
    "## Step 7 - Labeling sections with [Li et al.](http://doi.org/10.1109/TSE.2022.3224378)'s Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from model_li2022_tse import *\n",
    "\n",
    "v = Model1_IssueTracker_Li2022_TSE('../model/satd-issue_mul.hdf5', '../model/fasttext_issue_300.bin')\n",
    "\n",
    "issues_dataset = pd.read_csv('../data/dataset-backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda391",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset['td-label-li-tse'] = '-'\n",
    "len_dataset = len(issues_dataset)\n",
    "for j, i in enumerate(issues_dataset.index):\n",
    "    issues_dataset['td-label'][i] = v.classify_prob_comment(issues_dataset['text'][i])\n",
    "    clear_output()\n",
    "    log.info(f'Handling {i+1} out {len_dataset} lines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
