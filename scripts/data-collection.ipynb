{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c320907a-4bd9-40b6-b42f-74cef7691351",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db20f-d245-4e87-bad3-6fa0bfbedc21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2afaca70-74f2-4fcf-9a83-c01cfa46ad33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from ipylogging import DisplayHandler, HTMLFormatter\n",
    "\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.addHandler(handler)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade3e0-8048-4e7f-a0dd-dc651fa4a12e",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d761f18c-b3b8-4dab-84f7-9f9b56e8431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('../data')\n",
    "cache_dir_github = data_dir.joinpath('github')\n",
    "bots_dataset_path = data_dir.joinpath('bots-dataset.csv')\n",
    "bots_issues_dir = data_dir.joinpath('bots-issues')\n",
    "\n",
    "#github_token = open('../gh-token.txt','r').readlines()[0].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5713359-ff37-41f7-bb5b-061ce9f30868",
   "metadata": {},
   "source": [
    "## Step 1 - Collect bots from [Golzadeh et al.](https://zenodo.org/record/4000388)'s dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35605f-4609-44a4-a509-324ae27d6e32",
   "metadata": {},
   "source": [
    "Download dataset from [Golzadeh et al.](https://zenodo.org/record/4000388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f17e3ed-7f81-49e1-b7fa-6760e93d4ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "url_bot_dataset = \"https://zenodo.org/record/4000388/files/groundtruthbots.csv.gz\"\n",
    "path_bot_dataset = data_dir.joinpath('groundtruthbots.csv')\n",
    "\n",
    "gz_path, _ = urllib.request.urlretrieve(url_bot_dataset)\n",
    "with gzip.open(gz_path, \"rb\") as f_in, open(path_bot_dataset, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2369-880b-4120-8d2f-36ac77dab317",
   "metadata": {},
   "source": [
    "Extract bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd30b-d15c-4ad0-82e6-04965add1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "\n",
    "bot_dataset = load_csv_dataset(path_bot_dataset)\n",
    "bot_dataset = [{'account': row['account']} for row in bot_dataset if row['type'] == 'Bot']\n",
    "save_csv_dataset(bots_dataset_path, data=bot_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0c8d7-1d45-424f-a368-11f9cca98495",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2 - Collect the number of issues per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3475c3-72d8-4fad-bd90-6aec7b409b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "from minirig import GHRequests\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    try:\n",
    "        bot['issue_count'] = gh_api.get_number_issues_involving_user(bot['account'], force=True)\n",
    "    except:\n",
    "        bot['issue_count'] = 'na'\n",
    "\n",
    "bot_dataset.sort(reverse=True, key=lambda x: -1 if x['issue_count'] == 'na' else x['issue_count'])\n",
    "save_csv_dataset('../data/new-bot-dataset.csv', data=bot_dataset, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21537d-2805-4dca-881b-319aef79fa96",
   "metadata": {},
   "source": [
    "## Step 3 - Download issues for each bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8834e8-c314-416a-8157-da09afcaaa75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        \n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        \n",
    "        if not bot_issue_path.joinpath('json').exists():    \n",
    "            full_issue = gh_api.get_issue_info(issue['number'], owner, project)  \n",
    "            bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "vaadin_bot = [x for x in bot_dataset if x['account'] == 'vaadin-bot']\n",
    "\n",
    "for bot in vaadin_bot:\n",
    "    \n",
    "    for issue in gh_api.get_issues_involving_user(bot['account'], force= True):\n",
    "        \n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        \n",
    "        if not bot_issue_path.joinpath('json').exists():    \n",
    "            full_issue = gh_api.get_issue_info(issue['number'], owner, project)  \n",
    "            bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99eec13",
   "metadata": {},
   "source": [
    "## Step 4 - Download comments for each issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/','')).joinpath('comments')\n",
    "        bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        comments = gh_api.get_comments_per_issue(issue['number'], owner, project, force = True)           \n",
    "        with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "            json.dump(comments, f)\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        print(bot_issue_path)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc35d9",
   "metadata": {},
   "source": [
    "## Step 6 - Labeling sections with [Li et al.](http://doi.org/10.1007/s10664-022-10128-3)'s Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f73c907-9685-461d-bf74-db8883835506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 16:16:16.184872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/mambauser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from model_li2022_emse import *\n",
    "nltk.download('punkt')\n",
    "model1_li2022_emse = Model1_IssueTracker_Li2022_ESEM('../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5', \n",
    "                 '../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-word_embedding_file.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242591f5-6fc0-4d47-9cba-6cb28b5ab01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_issues_per_bot(bot, issue_count):\n",
    "    issues = []\n",
    "    cnt = 1\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                issue_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue)\n",
    "                try:\n",
    "                    with open(issue_path.joinpath('json')) as f:\n",
    "                        issue_file = json.load(f)\n",
    "                        yield {'path':issue_path, 'content':issue_file}\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "def load_issues_comments_per_bot(bot, issue_count):\n",
    "    issues = []\n",
    "    cnt = 1\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                comments_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue).joinpath('comments')\n",
    "                try:\n",
    "                    with open(comments_path.joinpath('json')) as f:\n",
    "                        comments_file = json.load(f)\n",
    "                        yield {'path':comments_path, 'content':issue_file, 'n-comments':len(comments_file)}\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "def store_labeled_issues(batch, predictions):\n",
    "    for i, issue in enumerate(batch):\n",
    "        issue['content']['td-label-li2022-emse'] = str(predictions[i])\n",
    "        with open(Path(issue['path']).joinpath('json'), 'w') as f:\n",
    "            json.dump(issue['content'], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff6ce18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-05-15 16:16:57,318</span> [<span style=\"font-weight: bold; color: dodgerblue\">INFO</span>] babel-bot: 2047/5641 (len batch:127)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object load_issues_per_bot at 0x7fb7b804d1c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_15536/1157556291.py\", line 22, in <module>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "ERROR:tornado.application:Exception in callback functools.partial(<bound method OutStream._flush of <ipykernel.iostream.OutStream object at 0x7fb82f1d6ad0>>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 95, in json_packer\n",
      "    return json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 20607: surrogates not allowed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 540, in _flush\n",
      "    self.session.send(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 850, in send\n",
      "    to_send = self.serialize(msg, ident)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 719, in serialize\n",
      "    content = self.pack(content)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 103, in json_packer\n",
      "    packed = json.dumps(\n",
      "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 20607: surrogates not allowed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-weight: bold; color: green\">2023-05-15 16:16:57,554</span> [<span style=\"font-weight: bold; color: crimson\">ERROR</span>] Exception in callback functools.partial(<bound method OutStream._flush of <ipykernel.iostream.OutStream object at 0x7fb82f1d6ad0>>)\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 95, in json_packer\n",
       "    return json.dumps(\n",
       "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 20607: surrogates not allowed\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
       "    ret = callback()\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/iostream.py\", line 540, in _flush\n",
       "    self.session.send(\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 850, in send\n",
       "    to_send = self.serialize(msg, ident)\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 719, in serialize\n",
       "    content = self.pack(content)\n",
       "  File \"/opt/conda/lib/python3.10/site-packages/jupyter_client/session.py\", line 103, in json_packer\n",
       "    packed = json.dumps(\n",
       "UnicodeEncodeError: 'utf-8' codec can't encode character '\\ud83d' in position 20607: surrogates not allowed"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "getWordVector(): incompatible function arguments. The following argument types are supported:\n    1. (self: fasttext_pybind.fasttext, arg0: fasttext_pybind.Vector, arg1: str) -> None\n\nInvoked with: <fasttext_pybind.fasttext object at 0x7fb82f0b41f0>, <fasttext_pybind.Vector object at 0x7fb79c75edf0>, '\\ud83d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m     batch\u001b[38;5;241m.\u001b[39mappend(issue)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 22\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1_li2022_emse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_sections_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     store_labeled_issues(batch, predictions)\n\u001b[1;32m     24\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []         \n",
      "File \u001b[0;32m/workspace/scripts/model_li2022_emse.py:104\u001b[0m, in \u001b[0;36mModel1_IssueTracker_Li2022_ESEM.label_sections_in_batch\u001b[0;34m(self, comments, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mClassify a single comment\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m:param comment:\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Prepare the comment for classification\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m input_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_comments(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m comments])\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Make predictions using the model\u001b[39;00m\n\u001b[1;32m    107\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(input_x, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/workspace/scripts/model_li2022_emse.py:104\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mClassify a single comment\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m:param comment:\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Prepare the comment for classification\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m input_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_comments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m comments])\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Make predictions using the model\u001b[39;00m\n\u001b[1;32m    107\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(input_x, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/workspace/scripts/model_li2022_emse.py:70\u001b[0m, in \u001b[0;36mModel1_IssueTracker_Li2022_ESEM.prepare_comments\u001b[0;34m(self, comment)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_embedding_cache:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(word)\n\u001b[0;32m---> 70\u001b[0m     word_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_word_embedding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word_embedding_cache[word] \u001b[38;5;241m=\u001b[39m word_embed\n\u001b[1;32m     72\u001b[0m     x_test\u001b[38;5;241m.\u001b[39mappend(word_embed)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fasttext/FastText.py:384\u001b[0m, in \u001b[0;36m_FastText.__getitem__\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_word_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fasttext/FastText.py:124\u001b[0m, in \u001b[0;36m_FastText.get_word_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    122\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dimension()\n\u001b[1;32m    123\u001b[0m b \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mVector(dim)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetWordVector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(b)\n",
      "\u001b[0;31mTypeError\u001b[0m: getWordVector(): incompatible function arguments. The following argument types are supported:\n    1. (self: fasttext_pybind.fasttext, arg0: fasttext_pybind.Vector, arg1: str) -> None\n\nInvoked with: <fasttext_pybind.fasttext object at 0x7fb82f0b41f0>, <fasttext_pybind.Vector object at 0x7fb79c75edf0>, '\\ud83d'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "batch_size = 128\n",
    "bots_dataset = pd.read_csv('../data/bots-dataset-labeling-management.csv',delimiter=',')\n",
    "\n",
    "for i in bots_dataset.index:\n",
    "    cnt = 1\n",
    "    if bots_dataset['label_finished'][i] == 'yes':\n",
    "        continue\n",
    "    bot = bots_dataset['account'][i]\n",
    "    batch = []\n",
    "    for issue in load_issues_per_bot(bot, bots_dataset['issue_count'][i]):\n",
    "        if cnt % 1500 == 0:\n",
    "            model1_li2022_emse.clear_model_session()\n",
    "        if 'body' in issue['content'].keys() and issue['content']['body'] != None and 'td-label-li2022-emse' not in issue['content'].keys():\n",
    "            batch.append(issue)\n",
    "        if len(batch) >= batch_size:\n",
    "            predictions = model1_li2022_emse.label_sections_in_batch(comments=[x['content']['body'] for x in batch], batch_size=batch_size)\n",
    "            store_labeled_issues(batch, predictions)\n",
    "            batch = []         \n",
    "        clear_output()\n",
    "        logging.info(f'{bot}: {cnt}/{bots_dataset[\"issue_count\"][i]} (len batch:{len(batch)})')\n",
    "        cnt += 1\n",
    "    if len(batch) > 0:\n",
    "        predictions = model1_li2022_emse.label_sections_in_batch(comments=[x['content']['body'] for x in batch], batch_size=batch_size)\n",
    "        store_labeled_issues(batch, predictions)\n",
    "        batch = []\n",
    "    bots_dataset['label_finished'][i] = 'yes'\n",
    "    bots_dataset.to_csv('../data/bots-dataset-labeling-management.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e3e92c-a231-4954-9a9b-d559c6666cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "print (\"\\ud83d\\ude04\".encode('utf-16','surrogatepass').decode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd2e6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 7 - Labeling sections with [Li et al.](http://doi.org/10.1109/TSE.2022.3224378)'s Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "from model_li2022_tse import *\n",
    "\n",
    "v = Model1_IssueTracker_Li2022_TSE('../model/satd-issue_mul.hdf5', '../model/fasttext_issue_300.bin')\n",
    "\n",
    "issues_dataset = pd.read_csv('../data/dataset-backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffda391",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset['td-label-li-tse'] = '-'\n",
    "len_dataset = len(issues_dataset)\n",
    "for j, i in enumerate(issues_dataset.index):\n",
    "    issues_dataset['td-label'][i] = v.classify_prob_comment(issues_dataset['text'][i])\n",
    "    clear_output()\n",
    "    log.info(f'Handling {i+1} out {len_dataset} lines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
