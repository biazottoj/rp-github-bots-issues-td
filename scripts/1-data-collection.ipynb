{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c320907a-4bd9-40b6-b42f-74cef7691351",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9db20f-d245-4e87-bad3-6fa0bfbedc21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2afaca70-74f2-4fcf-9a83-c01cfa46ad33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from ipylogging import DisplayHandler, HTMLFormatter\n",
    "\n",
    "handler = DisplayHandler()\n",
    "handler.setFormatter(HTMLFormatter())\n",
    "\n",
    "\n",
    "log = logging.getLogger()\n",
    "log.addHandler(handler)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade3e0-8048-4e7f-a0dd-dc651fa4a12e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d761f18c-b3b8-4dab-84f7-9f9b56e8431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('../data')\n",
    "cache_dir_github = data_dir.joinpath('github')\n",
    "bots_dataset_path = data_dir.joinpath('bots-dataset.csv')\n",
    "bots_issues_dir = data_dir.joinpath('bots-issues')\n",
    "\n",
    "\n",
    "github_token = open('../gh-token.txt','r').readlines()[0].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5713359-ff37-41f7-bb5b-061ce9f30868",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1 - Collect bots from [Golzadeh et al.](https://zenodo.org/record/4000388)'s dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35605f-4609-44a4-a509-324ae27d6e32",
   "metadata": {},
   "source": [
    "Download dataset from [Golzadeh et al.](https://zenodo.org/record/4000388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17e3ed-7f81-49e1-b7fa-6760e93d4ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "url_bot_dataset = \"https://zenodo.org/record/4000388/files/groundtruthbots.csv.gz\"\n",
    "path_bot_dataset = data_dir.joinpath('groundtruthbots.csv')\n",
    "\n",
    "gz_path, _ = urllib.request.urlretrieve(url_bot_dataset)\n",
    "with gzip.open(gz_path, \"rb\") as f_in, open(path_bot_dataset, \"wb\") as f_out:\n",
    "    f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2369-880b-4120-8d2f-36ac77dab317",
   "metadata": {},
   "source": [
    "Extract bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd30b-d15c-4ad0-82e6-04965add1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "\n",
    "bot_dataset = load_csv_dataset(path_bot_dataset)\n",
    "bot_dataset = [{'account': row['account']} for row in bot_dataset if row['type'] == 'Bot']\n",
    "save_csv_dataset(bots_dataset_path, data=bot_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0c8d7-1d45-424f-a368-11f9cca98495",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 2 - Collect the number of issues per bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3475c3-72d8-4fad-bd90-6aec7b409b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "from minirig import GHRequests\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    try:\n",
    "        bot['issue_count'] = gh_api.get_number_issues_involving_user(bot['account'], force=True)\n",
    "    except:\n",
    "        bot['issue_count'] = 'na'\n",
    "\n",
    "bot_dataset.sort(reverse=True, key=lambda x: -1 if x['issue_count'] == 'na' else x['issue_count'])\n",
    "save_csv_dataset('../data/new-bot-dataset.csv', data=bot_dataset, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21537d-2805-4dca-881b-319aef79fa96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 3 - Download issues for each bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8834e8-c314-416a-8157-da09afcaaa75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        \n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/',''))\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        \n",
    "        if not bot_issue_path.joinpath('json').exists():    \n",
    "            full_issue = gh_api.get_issue_info(issue['number'], owner, project)  \n",
    "            bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "            with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "                json.dump(full_issue, f)\n",
    "\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99eec13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 4 - Download comments for each issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import GHRequests, load_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "bot_dataset = load_csv_dataset(bots_dataset_path)\n",
    "gh_api = GHRequests(token=github_token,cache_dir=cache_dir_github)\n",
    "\n",
    "issues_errors = []\n",
    "\n",
    "for bot in bot_dataset:\n",
    "    cnt = 1\n",
    "    for issue in gh_api.get_issues_involving_user(bot['account']):\n",
    "        bot_issue_path = bots_issues_dir.joinpath(bot['account']).joinpath(issue['html_url'].replace('https://github.com/','')).joinpath('comments')\n",
    "        bot_issue_path.mkdir(parents=True, exist_ok=True)\n",
    "        owner, project = issue['repository_url'].replace('https://api.github.com/repos/', '').split('/')\n",
    "        comments = gh_api.get_comments_per_issue(issue['number'], owner, project, force = True)           \n",
    "        with open(bot_issue_path.joinpath('json'), 'w') as f:\n",
    "            json.dump(comments, f)\n",
    "        clear_output()\n",
    "        logging.info(f\"{bot['account']}: {cnt} of {bot['issue_count']}\")\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc35d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 5 - Labeling sections with [Li et al.](http://doi.org/10.1007/s10664-022-10128-3)'s Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242591f5-6fc0-4d47-9cba-6cb28b5ab01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import load_csv_dataset,  save_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from model_li2022_emse import *\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#model1_li2022_emse = Model1_IssueTracker_Li2022_ESEM('../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-weight_file.hdf5', \n",
    "                 #'../model/mode1-issue-tracker-li2022-emse/model1-issue-tracker-li2022-esem-word_embedding_file.bin')\n",
    "\n",
    "def load_issues_per_bot(bot, issue_count):\n",
    "    issues = []\n",
    "    cnt = 1\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                issue_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue)\n",
    "                with open(issue_path.joinpath('json')) as f:\n",
    "                    print(issue_path)\n",
    "                    try:\n",
    "                        issue_file = json.load(f)\n",
    "                        yield {'path':issue_path, 'content':issue_file}\n",
    "                    except:\n",
    "                        issues.append(f'{str(issue_path)}\\n')\n",
    "    with open(data_dir.joinpath('issues_error').joinpath(f'issue_error_{bot}.txt'), 'w') as f:\n",
    "        f.writelines(issues)\n",
    "        \n",
    "def store_labeled_issues(batch, predictions):\n",
    "    for i, issue in enumerate(batch):\n",
    "        issue['content']['td-label-li2022-emse'] = str(predictions[i])\n",
    "        with open(Path(issue['path']).joinpath('json'), 'w') as f:\n",
    "            json.dump(issue['content'], f)\n",
    "\n",
    "def load_issues_comments_per_bot(bot, issue_count):\n",
    "    issues = []\n",
    "    cnt = 1\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                comments_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue).joinpath('comments')\n",
    "                with open(comments_path.joinpath('json')) as f:\n",
    "                    comments_file = json.load(f)\n",
    "                    batch_item = {'path':comments_path, 'content':comments_file, 'n-comments':len(comments_file)}\n",
    "                    yield batch_item\n",
    "\n",
    "def prepare_comments_batch(batch):\n",
    "    batched_comments = []\n",
    "    for comments_file in batch:\n",
    "        for c in comments_file['content']:\n",
    "            batched_comments.append(c['body'])\n",
    "    return batched_comments\n",
    "\n",
    "def store_labeled_comments(batch, predictions):\n",
    "    pred_idx = 0\n",
    "    for comments in batch:\n",
    "        for i in range(int(comments['n-comments'])):       \n",
    "            comments['content'][i]['td-label-li2022-emse'] = str(predictions[pred_idx])\n",
    "            pred_idx += 1\n",
    "                       \n",
    "        with open(Path(comments['path']).joinpath('json'), 'w') as f:\n",
    "            json.dump(comments['content'], f)\n",
    "\n",
    "def check_if_labeled(comments):\n",
    "    for c  in comments['content']:\n",
    "        if isinstance(c, dict) and 'td-label-li2022-emse' not in c.keys():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6ce18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "bots_dataset = pd.read_csv('../data/bots-dataset-labeling-management.csv',delimiter=',')\n",
    "\n",
    "for i in bots_dataset.index:\n",
    "    cnt = 1\n",
    "    if bots_dataset['label_finished'][i] == 'yes':\n",
    "        continue\n",
    "    bot = bots_dataset['account'][i]\n",
    "    batch = []\n",
    "    for issue in load_issues_per_bot(bot, bots_dataset['issue_count'][i]):\n",
    "        model1_li2022_emse.clear_model_session()\n",
    "        if 'body' in issue['content'].keys() and issue['content']['body'] != None and 'td-label-li2022-emse' not in issue['content'].keys():\n",
    "            batch.append(issue)\n",
    "        if len(batch) >= batch_size:\n",
    "            predictions = model1_li2022_emse.label_sections_in_batch(comments=[x['content']['body'] for x in batch], batch_size=batch_size)\n",
    "            store_labeled_issues(batch, predictions)\n",
    "            batch = []         \n",
    "        clear_output()\n",
    "        logging.info(f'{bot}: {cnt}/{bots_dataset[\"issue_count\"][i]} (len batch:{len(batch)})')\n",
    "        cnt += 1\n",
    "    if len(batch) > 0:\n",
    "        predictions = model1_li2022_emse.label_sections_in_batch(comments=[x['content']['body'] for x in batch], batch_size=batch_size)\n",
    "        store_labeled_issues(batch, predictions)\n",
    "        batch = []\n",
    "    bots_dataset['label_finished'][i] = 'yes'\n",
    "    bots_dataset.to_csv('../data/bots-dataset-labeling-management.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f7f2e-0be7-4337-a627-b513dc83335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "bots_dataset = pd.read_csv('../data/bots-dataset-labeling-management.csv',delimiter=',')\n",
    "\n",
    "for i in bots_dataset.index:\n",
    "    cnt = 1\n",
    "    if bots_dataset['label_comments_finished'][i] == 'yes':\n",
    "        continue\n",
    "    bot = bots_dataset['account'][i]\n",
    "    batch = []\n",
    "    for comments in load_issues_comments_per_bot(bot, bots_dataset['issue_count'][i]):\n",
    "        if cnt == 3148:\n",
    "            continue\n",
    "        if comments == None or check_if_labeled(comments):\n",
    "            cnt += 1\n",
    "            clear_output()\n",
    "            logging.info(f'{bot}: {cnt}/{bots_dataset[\"issue_count\"][i]}')\n",
    "            continue\n",
    "        batch.append(comments)\n",
    "        batched_comments = []\n",
    "        if len(batch) >= batch_size:\n",
    "            model1_li2022_emse.clear_model_session()\n",
    "            batched_comments = prepare_comments_batch(batch)\n",
    "            predictions = []\n",
    "            predictions = model1_li2022_emse.label_sections_in_batch(batched_comments, batch_size=len(batched_comments))\n",
    "            store_labeled_comments(batch, predictions)\n",
    "            batch = []\n",
    "        clear_output()\n",
    "        logging.info(f'{bot}: {cnt}/{bots_dataset[\"issue_count\"][i]} (len batch:{len(batched_comments)})')\n",
    "        cnt += 1\n",
    "    if len(batch) > 1:\n",
    "        predictions = model1_li2022_emse.label_sections_in_batch(batched_comments, batch_size=len(batched_comments))\n",
    "        store_labeled_comments(batch, predictions)\n",
    "        batch = []\n",
    "        \n",
    "    bots_dataset['label_comments_finished'][i] = 'yes'\n",
    "    bots_dataset.to_csv('../data/bots-dataset-labeling-management.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f713d7-67bb-45f7-bc71-e4741ed88e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def get_owner_project(path):\n",
    "    return f\"{split_path[4]}_{split_path[5]}\"\n",
    "    \n",
    "def get_issue_json(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "project_data = {}\n",
    "for bot in filtered_bots_dataset:\n",
    "    cnt = 1\n",
    "    dataset = []\n",
    "    for path in bots_issues_dir.joinpath(bot['account']).rglob( '*json' ):\n",
    "        split_path = str(path).split('/')\n",
    "        issue_file = get_issue_json(path)        \n",
    "        if issue_file:\n",
    "            owner_project = get_owner_project(split_path)\n",
    "            if 'comments' in split_path:\n",
    "                for i, c in enumerate(issue_file):\n",
    "                    row = create_dataset_row(bot['account'], c, text_section='body', is_comment = True, comment_number = i, owner_project = owner_project)\n",
    "                    dataset.append(row)\n",
    "            else:\n",
    "                row_1 = create_dataset_row(bot['account'], issue_file, text_section='title', is_comment=False, owner_project=owner_project)\n",
    "                dataset.append(row_1)\n",
    "    \n",
    "                row_2 = create_dataset_row(bot['account'], issue_file, text_section='body', is_comment=False, owner_project=owner_project)\n",
    "                dataset.append(row_2)\n",
    "           \n",
    "            cnt += 1\n",
    "            clear_output()\n",
    "            print(path)\n",
    "            logging.info(f'Processing sectiong for {bot[\"account\"]} ({cnt})...')\n",
    "                \n",
    "    with open(data_dir.joinpath('datasets-per-bot').joinpath(f\"{bot['account']}.csv\"), 'w') as file:\n",
    "        clear_output()\n",
    "        logging.info(f'Writing csv file for {bot[\"account\"]}...')\n",
    "        fields = dataset[0].keys()\n",
    "        writer = csv.DictWriter(file, fieldnames = fields, escapechar='\\\\') \n",
    "        writer.writeheader() \n",
    "        writer.writerows(dataset)\n",
    "    logging.info('Finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b94ba-747b-4874-9f26-e29cba6ef310",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 6 - Generating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78693e-b7b7-4814-9cdc-f09600db9202",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Step 6.1 - Collecting the number of issue types per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b5fa5-546f-4801-812e-5e0dbcff2883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from minirig import load_csv_dataset, save_csv_dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_issues_per_bot(bot, issue_count):\n",
    "    issues = []\n",
    "    cnt = 1\n",
    "    for owner in os.listdir(bots_issues_dir.joinpath(bot)):\n",
    "        for project in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner)):\n",
    "            for issue in os.listdir(bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues')):\n",
    "                issue_path = bots_issues_dir.joinpath(bot).joinpath(owner).joinpath(project).joinpath('issues').joinpath(issue)\n",
    "                yield issue_path\n",
    "\n",
    "\n",
    "bots_more_100_issues = [x for x in load_csv_dataset(bots_dataset_path) if int(x['issue_count']) >= 100]\n",
    "bots_dataset_totals = []\n",
    "issues_not_labeled = []\n",
    "issue_error = []\n",
    "comments_error = []\n",
    "for bot_i in bots_more_100_issues:\n",
    "    cnt = 1\n",
    "    bot = bot_i['account']\n",
    "    row = {\"bot\":bot,\"n-issues-td\":0, \"n-issues-non-td\": 0, \"opened-satd\": 0, \"opened-non-satd\": 0, \"closed-satd\": 0, \"closed-non-satd\": 0, \"comments-satd\": 0, \"comments-non-satd\": 0}    \n",
    "    for issue_path in load_issues_per_bot(bot, bot_i['issue_count']):\n",
    "        try:\n",
    "            with open(issue_path.joinpath('json')) as f:\n",
    "                issue = json.load(f)\n",
    "                is_closed = issue['state'] == 'closed'\n",
    "        except:\n",
    "            issue_error.append(f'{str(issue_path)}\\n')\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            has_td = issue['td-label-li2022-emse'] == 'SATD'\n",
    "        except:\n",
    "            issues_not_labeled.append(issue_path)\n",
    "            continue\n",
    "\n",
    "        if has_td:\n",
    "            row['n-issues-td'] += 1\n",
    "        else:\n",
    "            row['n-issues-non-td'] += 1\n",
    "\n",
    "        if issue['user']['login'] == bot:\n",
    "            if has_td:\n",
    "                row['opened-satd'] += 1\n",
    "            else: \n",
    "                row['opened-non-satd'] += 1\n",
    "            \n",
    "        if is_closed and issue['closed_by'] != None:\n",
    "            if issue['closed_by']['login'] == bot:\n",
    "                if has_td:\n",
    "                    row['closed-satd'] += 1\n",
    "                else: \n",
    "                    row['closed-non-satd'] += 1\n",
    "        try:\n",
    "            f = open(issue_path.joinpath('comments').joinpath('json'))\n",
    "            comments = json.load(f)\n",
    "        except:\n",
    "            comments_error.append(str(issue_path)+'\\n')\n",
    "            continue\n",
    "\n",
    "        if type(comments) is list:\n",
    "            for c in comments:\n",
    "                if c['user']['login'] == bot:\n",
    "                    if has_td:\n",
    "                        row['comments-satd'] += 1\n",
    "                    else:\n",
    "                        row['comments-non-satd'] += 1\n",
    "                    break\n",
    "                    \n",
    "        elif type(comments) is dict:\n",
    "            if 'user' in comments.keys():\n",
    "                if comments['user']['login'] == bot:\n",
    "                    if has_td:\n",
    "                        row['comments-satd'] += 1\n",
    "                    else:\n",
    "                        row['comments-non-satd'] += 1\n",
    "            else:\n",
    "                comments_error.append(str(issue_path)+'\\n')\n",
    "                continue\n",
    "                \n",
    "        else:\n",
    "            comments_error.append(str(issue_path)+'\\n')\n",
    "            continue\n",
    "        \n",
    "                    \n",
    "        clear_output()\n",
    "        cnt += 1\n",
    "        print(bot,cnt,bot_i['issue_count'])   \n",
    "    bots_dataset_totals.append(row)\n",
    "    save_csv_dataset(data_dir.joinpath('bots-dataset-tasks.csv'), bots_dataset_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747b72d-6fed-4e46-8583-622acebd4d96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Step 6.2 - Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9b09019-2ef8-4bc6-995a-7c7c7bc6cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_dir.joinpath('bots-dataset-tasks.csv'))\n",
    "total_issues_td = df['total-issues-td'].sum()\n",
    "total_issues_non_td = df['total-issues-non-td'].sum()\n",
    "total_issues = total_issues_td + total_issues_non_td\n",
    "proportion_td = total_issues_td / total_issues\n",
    "proportion_non_td = total_issues_non_td / total_issues\n",
    "\n",
    "df['opened-satd-norm-bot'] = df['opened-satd']/(df['opened-satd'] + df['opened-non-satd'])\n",
    "df['opened-non-satd-norm-bot'] = df['opened-non-satd']/(df['opened-satd'] + df['opened-non-satd'])\n",
    "df['closed-satd-norm-bot'] = df['closed-satd']/(df['closed-satd'] + df['closed-non-satd'])\n",
    "df['closed-non-satd-norm-bot'] = df['closed-non-satd']/(df['closed-satd'] + df['closed-non-satd'])\n",
    "df['comments-satd-norm-bot'] = df['comments-satd']/(df['comments-satd'] + df['comments-non-satd'])\n",
    "df['comments-non-satd-norm-bot'] = df['comments-non-satd']/(df['comments-satd'] + df['comments-non-satd'])\n",
    "\n",
    "\n",
    "df['opened-satd-norm-pop'] = df['opened-satd-norm-bot']/proportion_td\n",
    "df['opened-non-satd-norm-pop'] = df['opened-non-satd-norm-bot']/proportion_non_td\n",
    "df['closed-satd-norm-pop'] = df['closed-satd-norm-bot']/proportion_td\n",
    "df['closed-non-satd-norm-pop'] = df['closed-non-satd-norm-bot']/proportion_non_td\n",
    "df['comments-satd-norm-pop'] = df['comments-satd-norm-bot']/proportion_td\n",
    "df['comments-non-satd-norm-pop'] = df['comments-non-satd-norm-bot']/proportion_non_td\n",
    "\n",
    "df.set_index('bot', inplace=True)\n",
    "df.to_csv(data_dir.joinpath('final-dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f32d07-2f4f-4d16-982c-e7dc7cdd7850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
